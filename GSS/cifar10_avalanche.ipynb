{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cal-05/anaconda3/envs/hspark/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import transforms\n",
    "\n",
    "from avalanche.training.supervised import GSS_greedy\n",
    "from avalanche.logging import InteractiveLogger, TensorboardLogger\n",
    "from avalanche.benchmarks.datasets import CIFAR10\n",
    "from avalanche.benchmarks.generators import nc_benchmark\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.evaluation.metrics import ExperienceAccuracy, ExperienceLoss, ExperienceForgetting, ExperienceTime, EpochAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "cudnn.enabled = False\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_set = CIFAR10('../Dataset/CIFAR10', train=True, download=True)\n",
    "test_set = CIFAR10('../Dataset/CIFAR10', train=False, download=True)\n",
    "\n",
    "transforms_group = dict(\n",
    "       train=(\n",
    "       transforms.Compose(\n",
    "              [\n",
    "              transforms.ToTensor(),\n",
    "              ]\n",
    "       ),\n",
    "       None,\n",
    "       ),\n",
    "       eval=(\n",
    "       transforms.Compose(\n",
    "              [\n",
    "              transforms.ToTensor(),\n",
    "              ]\n",
    "       ),\n",
    "       None,\n",
    "       )\n",
    ")\n",
    "\n",
    "train_set = AvalancheDataset(train_set, transform_groups=transforms_group, initial_transform_group=\"train\")\n",
    "test_set = AvalancheDataset(test_set, transform_groups=transforms_group, initial_transform_group=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "num_class = 10\n",
    "incremental = 5\n",
    "\n",
    "scenario = nc_benchmark(train_dataset=train_set,\n",
    "                        test_dataset=test_set,\n",
    "                        n_experiences=incremental,\n",
    "                        task_labels=False,\n",
    "                        seed=seed,\n",
    "                        shuffle=False\n",
    "                        )\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=False, num_classes=num_class)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-10 16:54:46.467791: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/cal-05/anaconda3/envs/hspark/lib/python3.8/site-packages/avalanche/training/plugins/evaluation.py:81: UserWarning: No benchmark provided to the evaluation plugin. Metrics may be computed on inconsistent portion of streams, use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "date = dt.datetime.now()\n",
    "date = date.strftime(\"%Y_%m_%d_%H_%M_%S\")\n",
    "\n",
    "interactive_logger = InteractiveLogger()\n",
    "tensor_logger = TensorboardLogger(\"logs_GSS-Greedy_cifar10_avalanche_\" + date)\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    EpochAccuracy(),\n",
    "    ExperienceAccuracy(),\n",
    "    ExperienceLoss(),\n",
    "    ExperienceForgetting(),\n",
    "    ExperienceTime(),\n",
    "    loggers=[interactive_logger, tensor_logger])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSS-Greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = 10\n",
    "eval_batch = 10\n",
    "epoch = 10\n",
    "\n",
    "strategies = GSS_greedy(model, optimizer, criterion, mem_size=200, mem_strength=10, input_size=[3, 32, 32], train_epochs=epoch, device=device, train_mb_size=train_batch, eval_mb_size=eval_batch, evaluator=eval_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start training on experience  0\n",
      "-- >> Start of training phase << --\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (32) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/cal-05/heonsung/Continual-Learning/GSS/cifar10_avalanche.ipynb ì…€ 8\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcal-05/home/cal-05/heonsung/Continual-Learning/GSS/cifar10_avalanche.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m experience \u001b[39min\u001b[39;00m scenario\u001b[39m.\u001b[39mtrain_stream:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcal-05/home/cal-05/heonsung/Continual-Learning/GSS/cifar10_avalanche.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mStart training on experience \u001b[39m\u001b[39m\"\u001b[39m, experience\u001b[39m.\u001b[39mcurrent_experience)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bcal-05/home/cal-05/heonsung/Continual-Learning/GSS/cifar10_avalanche.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     strategies\u001b[39m.\u001b[39;49mtrain(experience)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcal-05/home/cal-05/heonsung/Continual-Learning/GSS/cifar10_avalanche.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEnd training on experience \u001b[39m\u001b[39m\"\u001b[39m, experience\u001b[39m.\u001b[39mcurrent_experience)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bcal-05/home/cal-05/heonsung/Continual-Learning/GSS/cifar10_avalanche.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mComputing accuracy on the test set\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/site-packages/avalanche/training/templates/base_sgd.py:130\u001b[0m, in \u001b[0;36mBaseSGDTemplate.train\u001b[0;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    125\u001b[0m           experiences: Union[CLExperience,\n\u001b[1;32m    126\u001b[0m                              ExpSequence],\n\u001b[1;32m    127\u001b[0m           eval_streams: Optional[Sequence[Union[CLExperience,\n\u001b[1;32m    128\u001b[0m                                                 ExpSequence]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    129\u001b[0m           \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 130\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mtrain(experiences, eval_streams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    131\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluator\u001b[39m.\u001b[39mget_last_metrics()\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/site-packages/avalanche/training/templates/base.py:107\u001b[0m, in \u001b[0;36mBaseTemplate.train\u001b[0;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperience \u001b[39min\u001b[39;00m experiences:\n\u001b[1;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_before_training_exp(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_exp(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexperience, eval_streams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    108\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_training_exp(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_training(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/site-packages/avalanche/training/templates/base_sgd.py:178\u001b[0m, in \u001b[0;36mBaseSGDTemplate._train_exp\u001b[0;34m(self, experience, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stop_training \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_epoch(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    179\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_training_epoch(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/site-packages/avalanche/training/templates/base_sgd.py:237\u001b[0m, in \u001b[0;36mBaseSGDTemplate.training_epoch\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_before_forward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    236\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmb_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward()\n\u001b[0;32m--> 237\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_after_forward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    239\u001b[0m \u001b[39m# Loss & Backward\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion()\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/site-packages/avalanche/training/templates/base_sgd.py:296\u001b[0m, in \u001b[0;36mBaseSGDTemplate._after_forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_after_forward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 296\u001b[0m     trigger_plugins(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mafter_forward\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/site-packages/avalanche/training/utils.py:35\u001b[0m, in \u001b[0;36mtrigger_plugins\u001b[0;34m(strategy, event, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m strategy\u001b[39m.\u001b[39mplugins:\n\u001b[1;32m     34\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(p, event):\n\u001b[0;32m---> 35\u001b[0m         \u001b[39mgetattr\u001b[39;49m(p, event)(strategy, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/hspark/lib/python3.8/site-packages/avalanche/training/plugins/gss_greedy.py:278\u001b[0m, in \u001b[0;36mGSS_greedyPlugin.after_forward\u001b[0;34m(self, strategy, num_workers, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m     batch_sample_memory_cos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_each_batch_sample_sim(\n\u001b[1;32m    274\u001b[0m         strategy, grad_dims, mem_grads, updated_mb_x, updated_mb_y\n\u001b[1;32m    275\u001b[0m     )\n\u001b[1;32m    277\u001b[0m curr_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mext_mem_list_current_index\n\u001b[0;32m--> 278\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mext_mem_list_x[curr_idx : curr_idx \u001b[39m+\u001b[39;49m offset]\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mcopy_(\n\u001b[1;32m    279\u001b[0m     updated_mb_x\n\u001b[1;32m    280\u001b[0m )\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mext_mem_list_y[curr_idx : curr_idx \u001b[39m+\u001b[39m offset]\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mcopy_(\n\u001b[1;32m    282\u001b[0m     updated_mb_y\n\u001b[1;32m    283\u001b[0m )\n\u001b[1;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_score[curr_idx : curr_idx \u001b[39m+\u001b[39m offset]\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mcopy_(\n\u001b[1;32m    285\u001b[0m     batch_sample_memory_cos\n\u001b[1;32m    286\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (32) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "print(\"Starting experiment...\")\n",
    "results = []\n",
    "\n",
    "for experience in scenario.train_stream:\n",
    "    print(\"Start training on experience \", experience.current_experience)\n",
    "    strategies.train(experience)\n",
    "    print(\"End training on experience \", experience.current_experience)\n",
    "    print(\"Computing accuracy on the test set\")\n",
    "    results.append(strategies.eval(scenario.test_stream[:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('hspark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7200c97f9580f269913cae0521a31c8c9bc2a022b66e7eee6f7caa3cb908faad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
